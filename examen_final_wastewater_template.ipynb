{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Examen final – Modelos no lineales para pronóstico\n",
        "\n",
        "## Pronóstico de carga viral de SARS-CoV-2 en aguas residuales\n",
        "\n",
        "> Notebook plantilla listo para empezar. La idea es que rellenes los `TODO` con tu propio desarrollo, sin tener que pelearte con la estructura básica.\n",
        "\n",
        "### 1. Contexto general\n",
        "\n",
        "Desde la pandemia de COVID-19, la **vigilancia basada en aguas residuales** (wastewater-based epidemiology) se usa para monitorear virus respiratorios a nivel poblacional. En lugar de depender solo de pruebas diagnósticas individuales, se analizan muestras de plantas de tratamiento para cuantificar la cantidad de **ARN viral** presente, usando técnicas como RT-qPCR o RT-dPCR.\n",
        "\n",
        "El **National Wastewater Surveillance System (NWSS)** del CDC (Centers for Disease Control and Prevention) publica datos abiertos con la evolución de la carga viral de SARS-CoV-2 en aguas residuales para múltiples sitios y estados, actualizados de forma periódica. Estas series permiten detectar cambios en la circulación del virus incluso **varios días antes** que los cambios en casos clínicos, y capturan también infecciones asintomáticas.\n",
        "\n",
        "### 2. Problema a resolver\n",
        "\n",
        "En este proyecto se trabajará con una **serie de tiempo univariada** que representa la evolución de la carga viral de SARS-CoV-2 en aguas residuales para un sitio / región específica (por ejemplo, una planta de tratamiento o un estado). Para cada fecha de muestreo se tiene una medida de concentración de ARN viral normalizada o una métrica derivada.\n",
        "\n",
        "Objetivo principal:\n",
        "\n",
        "> **Construir y evaluar modelos de series de tiempo para pronosticar la carga viral de SARS-CoV-2 en aguas residuales en las próximas N semanas** a partir del histórico disponible.\n",
        "\n",
        "En términos prácticos:\n",
        "\n",
        "- La variable objetivo será una medida numérica asociada a la **carga viral** en aguas residuales (por ejemplo, una concentración normalizada o índice de actividad viral).\n",
        "- Trabajaremos con granularidad temporal según el dataset (diaria o semanal).\n",
        "- Usaremos ventanas deslizantes del tipo:\n",
        "  \n",
        "  \\[\n",
        "  [y_{t-w+1}, \\dots, y_t] \\rightarrow y_{t+1}\n",
        "  \\]\n",
        "\n",
        "### 3. Motivación\n",
        "\n",
        "1. **Salud pública y decisiones**: un buen pronóstico de la carga viral en aguas residuales permite anticipar aumentos de circulación antes de saturar hospitales o ver picos en los casos reportados.\n",
        "2. **Componente bio / genético real**: aunque el análisis final es una serie de tiempo agregada, el dato de origen viene de cuantificar **ARN viral** con técnicas moleculares. Es un ejemplo directo de cómo la biotecnología y la genómica se conectan con modelos predictivos.\n",
        "3. **Relevancia actual**: sistemas como NWSS siguen activos y se expanden a nuevos patógenos (influenza, RSV, mpox, etc.), por lo que el enfoque es reutilizable.\n",
        "\n",
        "### 4. TODO: personaliza el problema con tus palabras\n",
        "\n",
        "- [ ] Describe en 2–3 párrafos por qué este tema te interesa (bio, datos, salud pública, etc.).\n",
        "- [ ] Define tu horizonte de predicción: ¿N días? ¿N semanas? (p. ej. 14 o 28 días).\n",
        "- [ ] Explica qué tipo de decisiones o insights se podrían obtener a partir del pronóstico.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checklist del examen (mini guía)\n",
        "\n",
        "Esta sección resume lo que debes cubrir según la rúbrica, para que no se te pase nada:\n",
        "\n",
        "1. **Problema y datos**\n",
        "   - Explicar variable, contexto y objetivo del pronóstico.\n",
        "   - Describir bien la fuente de datos (CDC / NWSS) y el proceso de descarga automática.\n",
        "   - Hacer exploración inicial: gráficos, tendencia, posibles estacionalidades.\n",
        "\n",
        "2. **Preparación del dataset**\n",
        "   - Crear ventanas deslizantes sin fuga de información.\n",
        "   - Justificar cualquier transformación (log, escalamiento, etc.).\n",
        "   - Documentar cómo se hace el split en train / val / test.\n",
        "\n",
        "3. **Modelado**\n",
        "   - Baseline ingenuo (último valor) como referencia.\n",
        "   - Al menos un modelo no lineal (LSTM, CNN 1D, MLP, XGBoost, etc.).\n",
        "   - Describir arquitectura e hiperparámetros y por qué se eligieron.\n",
        "\n",
        "4. **Evaluación y resultados**\n",
        "   - Usar mínimo 3 métricas (MAE, RMSE, MAPE/sMAPE, etc.).\n",
        "   - Graficar real vs predicho en train/val/test o al menos en test.\n",
        "\n",
        "5. **Pronóstico y conclusiones**\n",
        "   - Mostrar pronóstico futuro (tabla + gráfica) y discutir si es coherente.\n",
        "   - Opcional: predicciones para fechas específicas.\n",
        "   - Redactar conclusiones claras, incluyendo limitaciones y mejoras futuras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importación de librerías, configuración de directorios y seeds globales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List, Optional, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "import seaborn as sns\n",
        "import requests\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Opcional: descomposición, ACF/PACF\n",
        "# import statsmodels.api as sm\n",
        "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Opcional: deep learning (elige 1 stack y descomenta)\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# OR\n",
        "# import torch\n",
        "# from torch import nn\n",
        "\n",
        "# Configuración de gráficos\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"muted\")\n",
        "\n",
        "# Directorios base\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "LANDING_DIR = DATA_DIR / \"landing\"   # en vez de 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "FIGURES_DIR = PROJECT_ROOT / \"figures\"\n",
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
        "\n",
        "for d in (DATA_DIR, LANDING_DIR, PROCESSED_DIR, FIGURES_DIR, MODELS_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Seed global\n",
        "GLOBAL_SEED = 42\n",
        "\n",
        "def set_global_seed(seed: int = 42):\n",
        "    \"\"\"Fija la seed para numpy, random y opcionalmente TF / Torch.\"\"\"\n",
        "    global GLOBAL_SEED\n",
        "    GLOBAL_SEED = seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # try:\n",
        "    #     tf.random.set_seed(seed)\n",
        "    # except Exception:\n",
        "    #     pass\n",
        "    # \n",
        "    # try:\n",
        "    #     torch.manual_seed(seed)\n",
        "    #     if torch.cuda.is_available():\n",
        "    #         torch.cuda.manual_seed_all(seed)\n",
        "    # except Exception:\n",
        "    #     pass\n",
        "\n",
        "set_global_seed(GLOBAL_SEED)\n",
        "\n",
        "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "print(f\"Seed global fijada en {GLOBAL_SEED}\")\n",
        "print(f\"LANDING_DIR: {LANDING_DIR}\")\n",
        "print(f\"PROCESSED_DIR: {PROCESSED_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers generales: ventanas, métricas, gráficas y pronósticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sliding_windows(\n",
        "    series: np.ndarray,\n",
        "    window_size: int,\n",
        "    horizon: int = 1,\n",
        "    stride: int = 1\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Crea ventanas deslizantes sin fuga de información.\"\"\"\n",
        "    series = np.asarray(series).astype(float)\n",
        "    T = len(series)\n",
        "    if T < window_size + horizon:\n",
        "        raise ValueError(\"Serie demasiado corta para el window_size y horizon dados.\")\n",
        "\n",
        "    X, y = [], []\n",
        "    last_start = T - window_size - horizon + 1\n",
        "    for start in range(0, last_start, stride):\n",
        "        end = start + window_size\n",
        "        target_end = end + horizon\n",
        "        X.append(series[start:end])\n",
        "        y.append(series[end:target_end])\n",
        "\n",
        "    X = np.stack(X)\n",
        "    y = np.stack(y)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def time_series_train_val_test_split(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    train_frac: float = 0.7,\n",
        "    val_frac: float = 0.15\n",
        ") -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"Divide en train / val / test respetando el orden temporal.\"\"\"\n",
        "    n = len(X)\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "    n_test = n - n_train - n_val\n",
        "    if n_test <= 0:\n",
        "        raise ValueError(\"Demasiados pocos datos para este split. Ajusta train_frac/val_frac.\")\n",
        "\n",
        "    X_train, y_train = X[:n_train], y[:n_train]\n",
        "    X_val, y_val = X[n_train:n_train + n_val], y[n_train:n_train + n_val]\n",
        "    X_test, y_test = X[n_train + n_val:], y[n_train + n_val:]\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n",
        "\n",
        "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Calcula MAE, RMSE y MAPE (maneja ceros con un pequeño epsilon).\"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    eps = 1e-8\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}\n",
        "\n",
        "\n",
        "def plot_time_series(dates, values, title: str = \"\", ylabel: str = \"\", ax=None):\n",
        "    dates = pd.to_datetime(dates)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.plot(dates, values, marker=\"o\", linewidth=1)\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(\"Fecha\")\n",
        "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m-%d\"))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_history_vs_pred(\n",
        "    dates,\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    title: str = \"Histórico vs predicción\"\n",
        "):\n",
        "    dates = pd.to_datetime(dates)\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.plot(dates, y_true, label=\"Real\", linewidth=1.5)\n",
        "    ax.plot(dates, y_pred, label=\"Predicho\", linewidth=1.5, linestyle=\"--\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Fecha\")\n",
        "    ax.set_ylabel(\"Valor\")\n",
        "    ax.legend()\n",
        "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m-%d\"))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_test_vs_pred(\n",
        "    dates_test,\n",
        "    y_test: np.ndarray,\n",
        "    y_pred_test: np.ndarray,\n",
        "    title: str = \"Test vs predicción\"\n",
        "):\n",
        "    return plot_history_vs_pred(dates_test, y_test, y_pred_test, title=title)\n",
        "\n",
        "\n",
        "def plot_future_forecast(\n",
        "    history_dates,\n",
        "    history_values,\n",
        "    future_dates,\n",
        "    future_preds,\n",
        "    title: str = \"Pronóstico futuro\"\n",
        "):\n",
        "    history_dates = pd.to_datetime(history_dates)\n",
        "    future_dates = pd.to_datetime(future_dates)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.plot(history_dates, history_values, label=\"Histórico\", linewidth=1.5)\n",
        "    ax.plot(future_dates, future_preds, label=\"Pronóstico\", linewidth=1.5, linestyle=\"--\", marker=\"o\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Fecha\")\n",
        "    ax.set_ylabel(\"Valor\")\n",
        "    ax.legend()\n",
        "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m-%d\"))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "def default_predict_fn(model, X):\n",
        "    if hasattr(model, \"predict\"):\n",
        "        return model.predict(X)\n",
        "    else:\n",
        "        return model(X)\n",
        "\n",
        "\n",
        "def recursive_forecast(\n",
        "    model,\n",
        "    last_window: np.ndarray,\n",
        "    n_future: int,\n",
        "    predict_fn=default_predict_fn\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Pronóstico recursivo a n_future pasos usando un modelo one-step-ahead.\"\"\"\n",
        "    window = np.asarray(last_window).reshape(1, -1)\n",
        "    preds = []\n",
        "    for _ in range(n_future):\n",
        "        y_hat = predict_fn(model, window)\n",
        "        y_hat = np.asarray(y_hat).reshape(-1)\n",
        "        y_next = float(y_hat[0])\n",
        "        preds.append(y_next)\n",
        "        # Deslizar ventana\n",
        "        window = np.roll(window, -1, axis=1)\n",
        "        window[0, -1] = y_next\n",
        "    return np.array(preds)\n",
        "\n",
        "\n",
        "def forecast_for_specific_dates(\n",
        "    model,\n",
        "    series: pd.Series,\n",
        "    future_dates: List,\n",
        "    window_size: int,\n",
        "    predict_fn=default_predict_fn\n",
        ") -> pd.Series:\n",
        "    \"\"\"Pronostica valores para fechas específicas futuras, asumiendo serie con índice datetime.\"\"\"\n",
        "    series = series.sort_index()\n",
        "    last_date = series.index[-1]\n",
        "    last_window = series.values[-window_size:]\n",
        "\n",
        "    future_dates = pd.to_datetime(pd.Index(future_dates))\n",
        "    future_dates = future_dates.sort_values()\n",
        "\n",
        "    steps_ahead = ((future_dates - last_date) / np.timedelta64(1, \"D\")).astype(int)\n",
        "    max_steps = steps_ahead.max()\n",
        "    if max_steps <= 0:\n",
        "        raise ValueError(\"Todas las fechas específicas deben ser posteriores al último dato.\")\n",
        "\n",
        "    all_future = recursive_forecast(model, last_window, int(max_steps), predict_fn=predict_fn)\n",
        "\n",
        "    values = []\n",
        "    for s in steps_ahead:\n",
        "        if s <= 0:\n",
        "            values.append(np.nan)\n",
        "        else:\n",
        "            values.append(all_future[int(s) - 1])\n",
        "\n",
        "    return pd.Series(values, index=future_dates, name=series.name)\n",
        "\n",
        "\n",
        "class NaiveLastValueModel:\n",
        "    \"\"\"Baseline ingenuo: pronostica el último valor de la ventana.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.fitted_ = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        last_vals = X[:, -1]\n",
        "        return last_vals.reshape(-1, 1)\n",
        "\n",
        "\n",
        "print(\"Helpers cargados: create_sliding_windows, regression_metrics, plot_* , recursive_forecast, forecast_for_specific_dates, NaiveLastValueModel.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datos NWSS: configuración de endpoints y descarga automática (landing)\n",
        "\n",
        "Vamos a usar el dataset público del CDC:\n",
        "\n",
        "- **ID:** `j9g8-acpt`\n",
        "- **Nombre:** CDC Wastewater Data for SARS-CoV-2\n",
        "- **Fuente:** data.cdc.gov\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoints oficiales del CDC para aguas residuales (SARS-CoV-2)\n",
        "\n",
        "CDC_WASTEWATER_CSV_URL = (\n",
        "    \"https://data.cdc.gov/api/views/j9g8-acpt/rows.csv?accessType=DOWNLOAD\"\n",
        ")\n",
        "\n",
        "# Endpoint API tipo SODA para consultas filtradas\n",
        "CDC_WASTEWATER_API_BASE = \"https://data.cdc.gov/resource/j9g8-acpt.csv\"\n",
        "\n",
        "# Archivo local en la capa landing\n",
        "CDC_WASTEWATER_FULL_CSV_PATH = LANDING_DIR / \"cdc_wastewater_sarscov2_full.csv\"\n",
        "\n",
        "print(\"CDC_WASTEWATER_CSV_URL:\", CDC_WASTEWATER_CSV_URL)\n",
        "print(\"CDC_WASTEWATER_API_BASE:\", CDC_WASTEWATER_API_BASE)\n",
        "print(\"CDC_WASTEWATER_FULL_CSV_PATH:\", CDC_WASTEWATER_FULL_CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_cdc_wastewater_full(\n",
        "    url: str = CDC_WASTEWATER_CSV_URL,\n",
        "    out_path: Path = CDC_WASTEWATER_FULL_CSV_PATH,\n",
        "    use_cache: bool = True,\n",
        "    chunk_size: int = 1_000_000,\n",
        ") -> Path:\n",
        "    \"\"\"Descarga el CSV completo de 'CDC Wastewater Data for SARS-CoV-2' a la capa landing.\n",
        "\n",
        "    Si use_cache=True y el archivo ya existe, no se vuelve a descargar.\n",
        "    \"\"\"\n",
        "    out_path = Path(out_path)\n",
        "\n",
        "    if use_cache and out_path.exists():\n",
        "        print(f\"[INFO] Usando archivo local en caché: {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "    print(f\"[INFO] Descargando datos desde:\\n  {url}\")\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=60) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "    except requests.RequestException as e:\n",
        "        print(\"[ERROR] Falló la descarga desde CDC.\")\n",
        "        print(\"Detalle:\", e)\n",
        "        raise\n",
        "\n",
        "    print(f\"[OK] Datos guardados en: {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def load_cdc_wastewater_full(\n",
        "    csv_path: Path = CDC_WASTEWATER_FULL_CSV_PATH,\n",
        "    parse_dates: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Carga el CSV completo de aguas residuales en un DataFrame.\n",
        "\n",
        "    NOTA: el dataset es grande; si solo necesitas un sitio, quizá convenga usar\n",
        "    la función de API filtrada en lugar de todo el CSV.\n",
        "    \"\"\"\n",
        "    csv_path = Path(csv_path)\n",
        "\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontró el archivo {csv_path}. \"\n",
        "            \"Ejecuta primero download_cdc_wastewater_full().\"\n",
        "        )\n",
        "\n",
        "    print(f\"[INFO] Cargando datos desde {csv_path} ...\")\n",
        "    df = pd.read_csv(csv_path, low_memory=False)\n",
        "\n",
        "    print(f\"[INFO] DataFrame cargado con shape: {df.shape}\")\n",
        "\n",
        "    if parse_dates:\n",
        "        # TODO: ajusta el nombre real de la columna de fecha según df.columns\n",
        "        date_candidates = [\n",
        "            \"sample_collect_date\",\n",
        "            \"collection_date\",\n",
        "            \"sample_date\",\n",
        "        ]\n",
        "        for col in date_candidates:\n",
        "            if col in df.columns:\n",
        "                print(f\"[INFO] Parseando columna de fecha: {col}\")\n",
        "                df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
        "                break\n",
        "        else:\n",
        "            print(\n",
        "                \"[WARN] No se reconoció automáticamente la columna de fecha. \"\n",
        "                \"Revisa df.columns y actualiza este bloque.\"\n",
        "            )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_cdc_wastewater_subset(\n",
        "    jurisdiction: Optional[str] = None,\n",
        "    sewershed_ids: Optional[Iterable[str]] = None,\n",
        "    min_date: Optional[str] = None,\n",
        "    max_date: Optional[str] = None,\n",
        "    limit: int = 50_000,\n",
        "    app_token: Optional[str] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Trae un subconjunto de datos desde la API SODA del CDC.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    jurisdiction : str, optional\n",
        "        Código de jurisdicción (ej. estado tipo 'CA', 'TX', etc.).\n",
        "    sewershed_ids : iterable of str, optional\n",
        "        Lista de IDs de sewershed a filtrar.\n",
        "    min_date, max_date : str, optional\n",
        "        Fechas 'YYYY-MM-DD' para filtrar por rango de colección.\n",
        "    limit : int\n",
        "        Límite de filas por llamada.\n",
        "    app_token : str, optional\n",
        "        Token de app de data.cdc.gov (opcional pero útil si haces muchas consultas).\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"$limit\": limit,\n",
        "    }\n",
        "\n",
        "    where_clauses = []\n",
        "\n",
        "    # TODO: ajusta nombres de columnas una vez veas df_raw.columns\n",
        "    date_col = \"sample_collect_date\"   # cambiar si el nombre es otro\n",
        "    jurisdiction_col = \"wwtp_jurisdiction\"  # ejemplo\n",
        "    sewershed_col = \"sewershed_id\"         # ejemplo\n",
        "\n",
        "    if jurisdiction:\n",
        "        where_clauses.append(\n",
        "            f\"upper({jurisdiction_col}) = '{jurisdiction.upper()}'\"\n",
        "        )\n",
        "\n",
        "    if sewershed_ids:\n",
        "        ids_str = \",\".join(f\"'{sid}'\" for sid in sewershed_ids)\n",
        "        where_clauses.append(f\"{sewershed_col} in ({ids_str})\")\n",
        "\n",
        "    if min_date:\n",
        "        where_clauses.append(f\"{date_col} >= '{min_date}'\")\n",
        "    if max_date:\n",
        "        where_clauses.append(f\"{date_col} <= '{max_date}'\")\n",
        "\n",
        "    if where_clauses:\n",
        "        params[\"$where\"] = \" AND \".join(where_clauses)\n",
        "\n",
        "    print(\"[INFO] Llamando a la API SODA de CDC con params:\")\n",
        "    for k, v in params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    headers = {}\n",
        "    if app_token:\n",
        "        headers[\"X-App-Token\"] = app_token\n",
        "\n",
        "    try:\n",
        "        r = requests.get(CDC_WASTEWATER_API_BASE, params=params, headers=headers, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        from io import StringIO\n",
        "        df = pd.read_csv(StringIO(r.text))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] No se pudo leer desde la API SODA.\")\n",
        "        print(\"Detalle:\", e)\n",
        "        raise\n",
        "\n",
        "    print(f\"[OK] Subconjunto recibido con shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"Funciones de descarga/carga CDC listas: download_cdc_wastewater_full, load_cdc_wastewater_full, fetch_cdc_wastewater_subset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selección de sewershed / región y construcción de la serie de tiempo\n",
        "\n",
        "Aquí vas a elegir **qué sitio / región específica** vas a modelar.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- [ ] Decidir si trabajarás con un **estado completo** o con un **sewershed específico**.\n",
        "- [ ] Definir rango de fechas (por ejemplo, de 2021-01-01 a la fecha más reciente disponible).\n",
        "- [ ] Seleccionar la columna que usarás como **fecha** y la que usarás como **target** (por ejemplo una concentración normalizada o índice de actividad viral).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: elige una de estas estrategias para obtener los datos de tu sitio/región\n",
        "\n",
        "# Opción A: usar el CSV completo (landing) y luego filtrar\n",
        "# download_cdc_wastewater_full(use_cache=True)\n",
        "# df_raw = load_cdc_wastewater_full()\n",
        "\n",
        "# Opción B: usar solo un subconjunto vía API (recomendable si ya tienes claro el sitio)\n",
        "# df_raw = fetch_cdc_wastewater_subset(\n",
        "#     jurisdiction=\"CA\",        # TODO: cambia por el estado/jurisdicción\n",
        "#     sewershed_ids=None,        # TODO: puedes pasar una lista de IDs concretos\n",
        "#     min_date=\"2021-01-01\",    # TODO: ajusta rango de fechas\n",
        "#     max_date=None,             # None = hasta la fecha más reciente\n",
        "#     limit=50000,\n",
        "#     app_token=None,            # Si tienes un app token, lo puedes poner aquí\n",
        "# )\n",
        "\n",
        "# TODO: descomenta UNA de las dos opciones de arriba y explora df_raw\n",
        "# df_raw.head()\n",
        "# df_raw.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: una vez identificado tu sitio / sewershed, construye aquí df_site\n",
        "\n",
        "# Ejemplo genérico (ajusta nombres reales de columnas):\n",
        "\n",
        "# date_col = \"sample_collect_date\"   # columna de fecha real\n",
        "# target_col = \"normalized_viral_load\"  # columna numérica que usarás como target\n",
        "# site_id_col = \"sewershed_id\"          # o la que corresponda\n",
        "\n",
        "# SEWERSHED_ID = \"<TU_ID_AQUI>\"         # TODO: pon el ID real que eliges\n",
        "\n",
        "# df_site = (\n",
        "#     df_raw\n",
        "#     .query(f\"{site_id_col} == @SEWERSHED_ID\")\n",
        "#     [[date_col, target_col]]\n",
        "#     .copy()\n",
        "# )\n",
        "\n",
        "# df_site[date_col] = pd.to_datetime(df_site[date_col], errors=\"coerce\")\n",
        "# df_site = df_site.dropna(subset=[date_col, target_col])\n",
        "# df_site = df_site.sort_values(date_col)\n",
        "\n",
        "# # Renombramos a columnas estándar para el resto del notebook\n",
        "# df_site = df_site.rename(columns={date_col: \"date\", target_col: \"target\"})\n",
        "\n",
        "# print(\"Rango de fechas:\", df_site[\"date\"].min(), \"→\", df_site[\"date\"].max())\n",
        "# print(\"Número de filas:\", len(df_site))\n",
        "# df_site.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploración y limpieza básica de la serie seleccionada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: ejecuta esta celda cuando df_site exista y tenga columnas ['date', 'target']\n",
        "\n",
        "# df_site = df_site.drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
        "# df_site = df_site.sort_values(\"date\")\n",
        "\n",
        "# print(\"Rango de fechas:\", df_site[\"date\"].min(), \"→\", df_site[\"date\"].max())\n",
        "# print(\"Número de filas (tras limpieza):\", len(df_site))\n",
        "# display(df_site.head())\n",
        "# display(df_site.describe())\n",
        "\n",
        "# # Tratamiento simple de NA (puedes mejorarlo si es necesario)\n",
        "# df_site[\"target\"] = df_site[\"target\"].interpolate().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
        "\n",
        "# # Gráfica rápida\n",
        "# plot_time_series(df_site[\"date\"], df_site[\"target\"], title=\"Serie completa – carga viral\", ylabel=\"target\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparación del dataset para modelado (ventanas deslizantes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de ventana y horizonte de predicción\n",
        "\n",
        "# TODO: ajusta según tu problema\n",
        "WINDOW_SIZE = 30   # días/semanas hacia atrás que ve el modelo\n",
        "HORIZON = 1        # predicción one-step-ahead\n",
        "TRAIN_FRAC = 0.7\n",
        "VAL_FRAC = 0.15\n",
        "\n",
        "# TODO: ejecuta cuando df_site exista\n",
        "# series = df_site.set_index(\"date\")[\"target\"].astype(float)\n",
        "# values = series.values\n",
        "# dates = series.index\n",
        "\n",
        "# X_all, y_all = create_sliding_windows(values, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
        "# print(\"X_all shape:\", X_all.shape)\n",
        "# print(\"y_all shape:\", y_all.shape)\n",
        "\n",
        "# (X_train, y_train), (X_val, y_val), (X_test, y_test) = time_series_train_val_test_split(\n",
        "#     X_all, y_all, train_frac=TRAIN_FRAC, val_frac=VAL_FRAC\n",
        "# )\n",
        "# print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# # Fechas correspondientes al target de cada ventana\n",
        "# target_dates = dates[WINDOW_SIZE:WINDOW_SIZE + len(y_all)]\n",
        "# train_dates = target_dates[: len(y_train)]\n",
        "# val_dates = target_dates[len(y_train): len(y_train) + len(y_val)]\n",
        "# test_dates = target_dates[len(y_train) + len(y_val):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escalamiento opcional (útil para redes neuronales)\n",
        "\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_val_scaled = scaler.transform(X_val)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # TODO: decide si vas a trabajar con las versiones escaladas o sin escalar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelos de pronóstico\n",
        "\n",
        "Vamos a definir primero un **baseline ingenuo**, y luego uno o más **modelos no lineales**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Baseline ingenuo (último valor) ===\n",
        "\n",
        "# TODO: ejecuta cuando tengas X_train, y_train, etc.\n",
        "\n",
        "# baseline = NaiveLastValueModel().fit(X_train, y_train)\n",
        "\n",
        "# y_train_pred = baseline.predict(X_train).reshape(-1)\n",
        "# y_val_pred = baseline.predict(X_val).reshape(-1)\n",
        "# y_test_pred = baseline.predict(X_test).reshape(-1)\n",
        "\n",
        "# y_train_true = y_train.reshape(-1)\n",
        "# y_val_true = y_val.reshape(-1)\n",
        "# y_test_true = y_test.reshape(-1)\n",
        "\n",
        "# metrics_train = regression_metrics(y_train_true, y_train_pred)\n",
        "# metrics_val = regression_metrics(y_val_true, y_val_pred)\n",
        "# metrics_test = regression_metrics(y_test_true, y_test_pred)\n",
        "\n",
        "# print(\"Baseline – métricas train:\", metrics_train)\n",
        "# print(\"Baseline – métricas val:\", metrics_val)\n",
        "# print(\"Baseline – métricas test:\", metrics_test)\n",
        "\n",
        "# plot_test_vs_pred(test_dates, y_test_true, y_test_pred, title=\"Baseline – Test vs predicción\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 1 – (ejemplo) LSTM / CNN / MLP\n",
        "\n",
        "Aquí puedes implementar tu primer modelo no lineal (LSTM, CNN 1D, MLP, etc.). Debes:\n",
        "\n",
        "- Definir la arquitectura y comentar para qué sirve cada parte.\n",
        "- Justificar hiperparámetros (número de capas, neuronas, tamaño de kernel, dropout, LR, etc.).\n",
        "- Mostrar `model.summary()` o equivalente y comentarlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementa aquí tu Modelo 1 (ejemplo LSTM con Keras)\n",
        "\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "# # Ejemplo: usar X_train_scaled con shape (n_samples, WINDOW_SIZE)\n",
        "# X_train_lstm = X_train_scaled.reshape(-1, WINDOW_SIZE, 1)\n",
        "# X_val_lstm = X_val_scaled.reshape(-1, WINDOW_SIZE, 1)\n",
        "# X_test_lstm = X_test_scaled.reshape(-1, WINDOW_SIZE, 1)\n",
        "\n",
        "# INPUT_SHAPE = (WINDOW_SIZE, 1)\n",
        "\n",
        "# model_1 = keras.Sequential([\n",
        "#     layers.Input(shape=INPUT_SHAPE),\n",
        "#     layers.LSTM(64, return_sequences=False),\n",
        "#     layers.Dense(32, activation=\"relu\"),\n",
        "#     layers.Dense(HORIZON)\n",
        "# ])\n",
        "\n",
        "# model_1.compile(\n",
        "#     loss=\"mse\",\n",
        "#     optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "#     metrics=[\"mae\"]\n",
        "# )\n",
        "\n",
        "# model_1.summary()\n",
        "\n",
        "# # TODO: entrena el modelo con early stopping\n",
        "# # history = model_1.fit(\n",
        "# #     X_train_lstm, y_train,\n",
        "# #     validation_data=(X_val_lstm, y_val),\n",
        "# #     epochs=100,\n",
        "# #     batch_size=32,\n",
        "# #     callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "# # )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación del Modelo 1\n",
        "\n",
        "# TODO: adapta esta celda a tu modelo (LSTM/CNN/etc.)\n",
        "\n",
        "# # y_train_pred_1 = model_1.predict(X_train_lstm).reshape(-1)\n",
        "# # y_val_pred_1 = model_1.predict(X_val_lstm).reshape(-1)\n",
        "# # y_test_pred_1 = model_1.predict(X_test_lstm).reshape(-1)\n",
        "\n",
        "# # y_train_true = y_train.reshape(-1)\n",
        "# # y_val_true = y_val.reshape(-1)\n",
        "# # y_test_true = y_test.reshape(-1)\n",
        "\n",
        "# # metrics_train_1 = regression_metrics(y_train_true, y_train_pred_1)\n",
        "# # metrics_val_1 = regression_metrics(y_val_true, y_val_pred_1)\n",
        "# # metrics_test_1 = regression_metrics(y_test_true, y_test_pred_1)\n",
        "\n",
        "# # print(\"Modelo 1 – métricas train:\", metrics_train_1)\n",
        "# # print(\"Modelo 1 – métricas val:\", metrics_val_1)\n",
        "# # print(\"Modelo 1 – métricas test:\", metrics_test_1)\n",
        "\n",
        "# # plot_test_vs_pred(test_dates, y_test_true, y_test_pred_1, title=\"Modelo 1 – Test vs predicción\")\n",
        "# # plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pronósticos futuros y fechas específicas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pronóstico a N_FUTURE pasos usando tu modelo final\n",
        "\n",
        "# TODO: elige tu modelo final (baseline, model_1, etc.)\n",
        "# final_model = model_1  # ejemplo\n",
        "\n",
        "# # Usamos la última ventana de toda la serie\n",
        "# # last_window = values[-WINDOW_SIZE:]\n",
        "# # N_FUTURE = 14\n",
        "# # future_preds = recursive_forecast(final_model, last_window, n_future=N_FUTURE)\n",
        "\n",
        "# # Fechas futuras (si la serie es diaria)\n",
        "# # last_date = dates[-1]\n",
        "# # future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=N_FUTURE, freq=\"D\")\n",
        "\n",
        "# # plot_future_forecast(dates, values, future_dates, future_preds, title=\"Pronóstico futuro – carga viral\")\n",
        "# # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicción para fechas específicas\n",
        "\n",
        "# TODO: define fechas futuras concretas (en formato YYYY-MM-DD)\n",
        "# specific_dates = [\n",
        "#     \"2025-01-01\",\n",
        "#     \"2025-02-01\",\n",
        "#     \"2025-03-01\",\n",
        "# ]\n",
        "\n",
        "# # preds_specific = forecast_for_specific_dates(\n",
        "# #     final_model,\n",
        "# #     series=series,\n",
        "# #     future_dates=specific_dates,\n",
        "# #     window_size=WINDOW_SIZE\n",
        "# # )\n",
        "\n",
        "# # preds_specific.to_frame(name=\"forecast\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones y hallazgos\n",
        "\n",
        "En esta sección deberás sintetizar:\n",
        "\n",
        "- Qué aprendiste de la serie de tiempo (tendencias, cambios, ruidos raros).\n",
        "- Qué modelo funcionó mejor y por qué.\n",
        "- En qué condiciones falla tu modelo (picos, cambios de régimen, datos faltantes, etc.).\n",
        "- Qué tan útil es el pronóstico para la toma de decisiones en el contexto de vigilancia en aguas residuales.\n",
        "- Qué harías diferente con más tiempo (más features, tuning más fino, otros modelos, etc.).\n",
        "\n",
        "Aquí también puedes dejar apuntes para tu README y tus diapositivas finales.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}